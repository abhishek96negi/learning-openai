{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f43d00a",
   "metadata": {},
   "source": [
    "Vector embeddings are a way to represent complex data like words or images as a set of numbers with fixed length. The important relationships between the data points are preserved in this representation where each dimension of the vector corresponds to a specific feature or attribute of the data. This is done using techniques like Word2Vec or GloVe for text and autoencoders for images. \n",
    "\n",
    "Once the vector embeddings are created, they can be used for a variety of tasks, such as:\n",
    "\n",
    "1. **Semantic similarity**: Measuring the similarity between words or phrases based on their vector representations.\n",
    "\n",
    "2. **Clustering**: Grouping similar data points together based on their vector embeddings.\n",
    "\n",
    "3. **Classification**: Training a machine learning model to classify data points based on their vector embeddings.\n",
    "\n",
    "4. **Visualization**: Plotting the vector embeddings in a lower-dimensional space to visualize the relationships between the data points.\n",
    "\n",
    "Vector embeddings have become a fundamental building block of many modern machine learning algorithms and have enabled significant progress in natural language processing, computer vision, and other areas of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11362c",
   "metadata": {},
   "source": [
    "![vector_embedding](../image/vector%20embedding/vector_embedding.png)\n",
    "\n",
    "                The process of creating vector embeddings from different types of data: Audio, Text, Video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e86e7c0",
   "metadata": {},
   "source": [
    "![vector_embedding2](../image/vector%20embedding/vector_embedding2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26508e",
   "metadata": {},
   "source": [
    "When we represent real-world objects and concepts such as images, audio recordings, news articles, user profiles, weather patterns, and political views as vector embeddings, the semantic similarity of these objects and concepts can be quantified by how close they are to each other as points in vector spaces. Vector embedding representations are thus suitable for common machine learning tasks such as clustering, recommendation, and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d7e42",
   "metadata": {},
   "source": [
    "![vector_embedding3](../image/vector%20embedding/vector_embedding3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479697cd",
   "metadata": {},
   "source": [
    "Embeddings are useful in tasks like clustering, recommendation, and classification because they measure similarity between objects in a more efficient way than traditional methods like one-hot encoding. One-hot encoding creates a column for every category, resulting in a sparse representation with lots of zeros. This can be problematic as the dataset grows in size and becomes computationally expensive to use. In contrast, embeddings transform categorical values into numerical values by representing them as vectors in a high-dimensional space, allowing for more efficient similarity measurements and better performance in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858436f6",
   "metadata": {},
   "source": [
    "![vector_embedding4](../image/vector%20embedding/vector_embedding4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42604",
   "metadata": {},
   "source": [
    "When dealing with a large dataset like the entire English language, one-hot encoding becomes impractical as it creates a representation with too many columns. Vector embeddings provide a fixed size representation that is more efficient and dense. The model creates a vector with fewer bytes and more information. This makes it computationally less expensive to use. Vector embeddings can be used in various tasks like reverse image search, chatbots, Q&A, and recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb3114",
   "metadata": {},
   "source": [
    "## Creating Vector Embeddings\n",
    "Machine learning models require numerical representations of data like text or images to understand them. Prior to machine learning, these representations were created manually through [feature engineering](https://www.kaggle.com/learn/feature-engineering). With deep learning, the model learns non-linear feature interactions automatically. Each layer focuses on a different aspect of the input data, creating new representations of it. This is how vector embeddings are created. For images, models like [convolutional neural networks (CNNs)](https://en.wikipedia.org/wiki/Convolutional_neural_network) can be used, and for audio, image embedding transformations can be used on the audio frequencies visual representation (such as its [spectrogram](https://en.wikipedia.org/wiki/Spectrogram))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbe2af",
   "metadata": {},
   "source": [
    "## Image Embedding with a Convolutional Neural Network\n",
    "In this example, images are represented by a matrix of integer values ranging from `0` to `255`, where `0` represents black and `255` represents white. The image is displayed in greyscale and has a corresponding matrix representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b705f1",
   "metadata": {},
   "source": [
    "![vector_embedding5](../image/vector%20embedding/vector_embedding5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccc25c",
   "metadata": {},
   "source": [
    "In image data, matrix representations capture the semantic information of pixel neighborhoods but are sensitive to transformations. Convolutional Neural Networks (CNNs) process visual data through hierarchical small local sub-inputs called receptive fields. Each neuron in each network layer processes a specific receptive field from the former layer. Each layer applies a convolution on the receptive field or reduces the input size, which is called subsampling. The resulting vector embedding is received via a fully connected layer. CNNs transform images into embeddings, which are more robust and can be used as inputs for further learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7918926b",
   "metadata": {},
   "source": [
    "![vector_embedding6](../image/vector%20embedding/vector_embedding6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1bfb5",
   "metadata": {},
   "source": [
    "The process of creating vector embeddings involves using deep learning models, such as Convolutional Neural Networks (CNNs), to transform raw data into a fixed-size representation that can be stored and used for various purposes. CNNs process inputs through small, local sub-inputs called receptive fields, and each neuron in each layer processes a specific receptive field from the previous layer. The weights of the embedding model are optimized using a large set of labeled data, so that images with the same labels are embedded closer together than images with different labels. Once the embedding model is learned, it can be used to transform new, unseen images into vectors that can be stored and compared to retrieve similar images. Vector embeddings can be created for any kind of data using different models/methods. The ResNet model, for example, is a CNN commonly used for image-related tasks and is trained to predict which of 1000 classes an object in an image belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd9745",
   "metadata": {},
   "source": [
    "![vector_embedding7](../image/vector%20embedding/vector_embedding7.png)\n",
    "\n",
    "                            Diagram of a simple Convolutional Neural Network (CNN)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2b169",
   "metadata": {},
   "source": [
    "A wealth of pre-trained models exist that can easily be used for creating vector embeddings. The [Huggingface Model Hub](https://huggingface.co/models) contains many models that can created embeddings for different types of data. For example, the [all-MiniLM-L6-v2 Model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) is hosted and runnable online, no expertise or install required.\n",
    "\n",
    "Packages like `sentence_transformers`, also from HuggingFace, provide easy-to-use models for tasks like semantic similarity search, visual search, and many others. To create embeddings with these models, only a few lines of Python are needed:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5846f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "  \"That is a very happy Person\",\n",
    "  \"That is a Happy Dog\",\n",
    "  \"Today is a sunny day\"\n",
    "]\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1953160c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00248324,  0.09151708,  0.0483862 , ..., -0.02641124,\n",
       "        -0.07529837,  0.02803207],\n",
       "       [ 0.00504994,  0.06316981,  0.01415729, ...,  0.04035437,\n",
       "         0.07584121,  0.09087349],\n",
       "       [-0.01629127,  0.10406605,  0.09740778, ...,  0.00676723,\n",
       "        -0.0878846 ,  0.03404384]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45019ff",
   "metadata": {},
   "source": [
    "## Vector Embeddings for Semantic Similarity Search\n",
    "Semantic Similarity Search is the process by which pieces of text are compared in order to find which contain the most similar meaning. While this might seem easy for an average human being, languages are quite complex. Distilling unstructured text data down into a format that a Machine Learning model can understand has been the subject of study for many Natural Language Processing researchers.\n",
    "\n",
    "Vector Embeddings provide a method for anyone, not just NLP researcher or data scientists, to perform semantic similarity search. They provide a meaningful, computationally efficient, numerical representation that can be created by pre-trained models “out of the box”. Below, an example of semantic similarity is shown that outlines the vector embeddings created with the sentence_transformers library shown above.\n",
    "\n",
    "Let’s take the following sentences:\n",
    "\n",
    "* \"That is a happy dog\"\n",
    "\n",
    "* \"That is a very happy person\"\n",
    "\n",
    "* \"Today is a sunny day\"\n",
    "\n",
    "Each of these sentences can be transformed into a vector embedding. Below, a simplified representation highlights the position of these example sentences in 2-dimensional vector space relative to one another. This is useful in order to visually gauge how effective our embeddings represent the semantic meaning of text. More on that below.\n",
    "\n",
    "A simplified plot of vector embeddings projected into 2 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaecc6ce",
   "metadata": {},
   "source": [
    "![vector_embedding8](../image/vector%20embedding/vector_embedding8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ad0fd",
   "metadata": {},
   "source": [
    "Assume we want to compare these sentences to “That is a happy person”. First, we create the vector embedding for the query sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9152ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# create the vector embedding for the query\n",
    "query_embedding = model.encode(\"That is a happy person\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a339cc16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.38769369e-02,  9.19415876e-02,  4.87012975e-02, -3.48836109e-02,\n",
       "       -6.48291782e-02, -2.66857557e-02,  1.34293348e-01, -6.91500213e-03,\n",
       "        6.44351020e-02, -5.82762714e-03,  8.87372196e-02, -1.62496660e-02,\n",
       "       -2.54945718e-02,  4.83907619e-03,  6.14902377e-03,  1.55436397e-02,\n",
       "       -5.95202409e-02, -3.20247486e-02,  1.41185373e-02,  2.05600355e-03,\n",
       "       -1.00310810e-01, -2.04243511e-03, -2.08597239e-02,  9.96055175e-03,\n",
       "       -1.69836041e-02, -1.64660122e-02,  4.00910266e-02, -2.72038789e-03,\n",
       "        8.66090879e-02,  6.33227378e-02, -2.68441048e-02, -2.35456694e-02,\n",
       "        1.09181821e-01,  2.25531738e-02, -3.85773554e-02,  1.94851663e-02,\n",
       "       -3.15520167e-02,  1.68708675e-02, -9.62975994e-03,  2.02890839e-02,\n",
       "       -1.82441864e-02,  1.77636892e-02,  1.86448190e-02,  1.22921271e-02,\n",
       "       -2.05458654e-03, -3.49595062e-02,  6.22536428e-02, -4.34290916e-02,\n",
       "        7.87903816e-02, -2.45035384e-02, -1.76689737e-02,  2.36276314e-02,\n",
       "       -5.71010821e-02, -8.24188441e-02,  6.16710968e-02,  6.15470558e-02,\n",
       "        9.44836065e-02, -1.42196864e-02,  4.13832404e-02,  4.30517681e-02,\n",
       "        4.59738858e-02,  6.99920431e-02, -4.54356670e-02, -2.98021268e-02,\n",
       "        7.19526857e-02,  1.46305878e-02, -4.86853085e-02, -2.72158217e-02,\n",
       "       -3.28596309e-02, -1.28150061e-02, -4.75828312e-02,  5.28202727e-02,\n",
       "        5.60503267e-02, -6.49439637e-03, -1.96329672e-02, -1.01571474e-02,\n",
       "        3.80518995e-02,  3.32267396e-02, -5.05527807e-03,  1.20274432e-01,\n",
       "        4.66576554e-02, -8.74283910e-02,  4.54903208e-03, -3.56090702e-02,\n",
       "       -3.44881676e-02, -2.38415077e-02,  1.97377661e-03, -3.43453176e-02,\n",
       "       -1.67489555e-02, -7.50820013e-03, -4.67805751e-02,  1.04574265e-03,\n",
       "        4.23300341e-02, -4.45300490e-02, -7.57928863e-02, -5.76891825e-02,\n",
       "       -6.08473048e-02,  1.77985411e-02, -6.93968534e-02,  7.32330829e-02,\n",
       "       -3.50507349e-02,  2.65310146e-02,  7.24961534e-02,  2.84561664e-02,\n",
       "        3.05621698e-02,  3.86910960e-02, -4.78830449e-02,  6.10319786e-02,\n",
       "        1.32104540e-02, -7.59307221e-02, -7.49388188e-02, -1.67037155e-02,\n",
       "        5.71596138e-02,  4.13788073e-02,  3.13882194e-02,  4.74488363e-02,\n",
       "       -1.03704385e-01,  6.55601695e-02,  3.74469832e-02, -5.83174638e-02,\n",
       "        2.64367983e-02,  3.57297137e-02,  6.07181415e-02,  3.27459015e-02,\n",
       "        1.23732705e-02, -8.32332894e-02,  2.23983843e-02, -5.19428289e-33,\n",
       "        6.17095968e-03,  3.94439995e-02,  4.69180644e-02, -8.30025226e-03,\n",
       "       -1.06798392e-02,  4.63808440e-02, -3.34496610e-02, -5.35777472e-02,\n",
       "       -4.36696932e-02,  1.19634867e-02, -1.48859546e-01, -2.08892934e-02,\n",
       "       -8.08392242e-02, -1.89027712e-02, -6.54326752e-02, -4.31383885e-02,\n",
       "       -6.01993129e-02,  4.54058982e-02, -5.61259799e-02,  3.81984599e-02,\n",
       "       -5.51633500e-02, -1.37762772e-02,  5.13640642e-02, -1.14175659e-02,\n",
       "       -2.13752929e-02,  1.21046323e-02,  9.47200358e-02, -2.84542646e-02,\n",
       "        3.89723256e-02,  5.82058821e-03, -7.74008408e-02,  4.59774993e-02,\n",
       "        3.16120498e-02, -6.24880046e-02, -6.43921569e-02, -1.24555707e-01,\n",
       "       -1.58593617e-02, -5.95163070e-02, -6.02860264e-02, -3.80659057e-03,\n",
       "       -3.43342721e-02,  3.96854989e-02, -3.17058037e-03,  5.93921766e-02,\n",
       "       -6.08647540e-02,  3.86578068e-02,  4.19698544e-02, -1.65120140e-02,\n",
       "        6.67832121e-02, -4.80096675e-02,  2.72241626e-02, -3.97528894e-02,\n",
       "        5.26158139e-02, -6.52634650e-02, -2.78338436e-02, -4.41417694e-02,\n",
       "        6.10613376e-02,  4.53412682e-02,  4.67405319e-02,  3.92431207e-02,\n",
       "        1.58241950e-02,  3.48532721e-02, -1.19336359e-01, -2.88092811e-02,\n",
       "        4.37335558e-02,  3.25751752e-02,  3.70548628e-02,  1.17492983e-02,\n",
       "        5.44685312e-02,  3.00375223e-02,  4.38501164e-02,  6.16271347e-02,\n",
       "        6.66532591e-02, -4.15174142e-02, -9.66064185e-02,  1.40935685e-02,\n",
       "       -9.27283056e-03, -7.02966899e-02, -2.16884278e-02, -1.07149789e-02,\n",
       "        7.31315091e-02, -2.20018663e-02, -1.56755745e-02, -6.99024741e-03,\n",
       "        1.56623516e-02, -2.47429069e-02, -2.26411801e-02, -7.02954382e-02,\n",
       "       -8.56185704e-02,  6.62742481e-02,  1.03612684e-01,  7.74609763e-03,\n",
       "        7.64575973e-02, -1.73363816e-02,  1.53809385e-02,  3.21295814e-33,\n",
       "        1.73293557e-02,  2.99649425e-02, -3.56900468e-02,  4.31125015e-02,\n",
       "        1.83658656e-02, -4.46689595e-03, -2.68976670e-03,  7.90846273e-02,\n",
       "       -8.57394785e-02,  1.08819708e-01,  9.64177847e-02, -4.70401086e-02,\n",
       "        9.90357623e-02,  3.19683068e-02,  1.89494602e-02,  3.59086134e-02,\n",
       "        4.41933498e-02,  1.65231805e-02, -2.21669655e-02,  5.92988133e-02,\n",
       "       -4.19053175e-02,  8.93903300e-02, -1.82972942e-02,  4.58531640e-02,\n",
       "       -3.36282700e-02,  4.76655737e-02,  5.25344312e-02, -9.00268331e-02,\n",
       "       -5.56840040e-02, -4.73977439e-02, -3.32043599e-03,  4.28303070e-02,\n",
       "       -7.02446699e-02, -1.42720461e-01,  3.38039920e-02, -1.18519301e-02,\n",
       "        1.48203485e-02,  4.72278297e-02, -3.10829375e-02,  2.43093725e-02,\n",
       "        1.41144795e-02,  4.01938558e-02,  2.53381990e-02,  3.75384875e-02,\n",
       "        2.79220100e-03,  1.46042965e-02,  7.54851624e-02, -5.27110994e-02,\n",
       "        1.58576574e-02,  9.32186097e-02, -4.33174111e-02, -4.24879268e-02,\n",
       "        2.05920748e-02,  4.91771586e-02,  1.36328414e-02, -1.72756575e-02,\n",
       "        1.02956528e-02, -7.08289593e-02,  5.93957826e-02, -5.88189475e-02,\n",
       "       -1.20467015e-01, -8.38990882e-03,  2.81796064e-02,  4.12777625e-02,\n",
       "        4.69840579e-02, -3.28420624e-02, -8.48039612e-02,  2.43585394e-03,\n",
       "       -5.72967231e-02,  1.06117632e-02,  5.97796142e-02, -5.44465594e-02,\n",
       "       -7.34235570e-02,  9.48531739e-03, -2.63451952e-02, -7.60596171e-02,\n",
       "        9.47527681e-03, -7.95020256e-04,  6.70059696e-02, -7.12489933e-02,\n",
       "       -7.09799901e-02, -3.56088416e-03,  5.92745841e-02,  3.88119519e-02,\n",
       "       -3.32118347e-02, -7.79238790e-02, -4.91814082e-03, -4.51408029e-02,\n",
       "       -1.09366863e-03,  5.30922823e-02,  2.97328047e-02,  2.74264831e-02,\n",
       "       -4.40770015e-02, -2.05070265e-02,  3.64419981e-03, -1.77887411e-08,\n",
       "       -4.56476584e-03, -4.97798361e-02, -6.79318756e-02, -9.41252336e-02,\n",
       "        4.32089753e-02,  1.47966910e-02,  1.15254812e-01, -1.06512390e-01,\n",
       "       -3.01295556e-02,  4.36999090e-02,  3.77380215e-02,  4.90806326e-02,\n",
       "        2.20075455e-02, -4.08373848e-02,  2.16149352e-02, -3.01952083e-02,\n",
       "        1.08728617e-01, -1.07205585e-02, -7.04815006e-03,  8.96152556e-02,\n",
       "        6.69415295e-02, -1.61226690e-02,  3.05078235e-02,  1.02114208e-01,\n",
       "       -7.70330057e-02, -3.31942248e-03, -8.29842091e-02, -2.72438414e-02,\n",
       "       -5.30076101e-02,  4.37176004e-02,  1.04870815e-02,  4.05125320e-03,\n",
       "       -2.96062641e-02, -7.09691225e-03,  9.63719413e-02,  7.06569999e-02,\n",
       "        1.32734273e-02, -2.20465865e-02,  4.45611440e-02,  7.43881464e-02,\n",
       "       -5.34378774e-02,  2.74968073e-02, -7.46114999e-02,  1.03844397e-01,\n",
       "       -6.21492267e-02, -2.62251534e-02,  3.56995463e-02, -4.12943326e-02,\n",
       "       -2.64705922e-02,  3.01117487e-02,  7.83537924e-02, -7.31024332e-03,\n",
       "       -4.43922281e-02,  8.53936821e-02, -3.58672962e-02, -1.11069486e-01,\n",
       "        3.51418890e-02,  1.81602687e-02, -5.18307090e-02, -1.23945931e-02,\n",
       "       -9.36717819e-03, -1.43925827e-02, -2.75497641e-02,  4.47581969e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c40331f",
   "metadata": {},
   "source": [
    "Next, we need to compare the distance between our query vector embedding and the vector embeddings in our dataset.\n",
    "\n",
    "There are many ways to calculate the distance between vectors. Each has their own benefits and drawbacks when it comes to semantic search, but we will save that for a separate post. Below some of the common distance metrics are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7a636",
   "metadata": {},
   "source": [
    "![vector_embedding9](../image/vector%20embedding/vector_embedding9.png)\n",
    "                        \n",
    "                        Distance metrics used in calculating vector similarity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbf37cb",
   "metadata": {},
   "source": [
    "For this example, we will use the cosine similarity which measures the distance between the inner product space of two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d396596",
   "metadata": {},
   "source": [
    "![vector_embedding10](../image/vector%20embedding/vector_embedding10.png)            \n",
    "            \n",
    "                                    Formula for Cosine Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac3616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python, this looks like\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a646e460",
   "metadata": {},
   "source": [
    "Running this calculation between our query vector and the other three vectors in the plot above, we can determine how similar the sentences are to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f4e847",
   "metadata": {},
   "source": [
    "![vector_embedding11](../image/vector%20embedding/vector_embedding11.png)\n",
    "\n",
    "\n",
    "                    \n",
    "            2D plot showing the cosine similarity between the vector embeddings created from our sentences earlier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16e98a",
   "metadata": {},
   "source": [
    "As you might have assumed, “That is a very happy person” is the most similar sentence to “That is a happy person”. This example captures only one of many possible use cases for vector embeddings: Semantic Similarity Search\n",
    "\n",
    "The Python code to run this entire example is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94b9a400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: That is a happy person\n",
      "That is a very happy person  -> similarity score =  0.9429151\n",
      "That is a happy dog  -> similarity score =  0.69457734\n",
      "Today is a sunny day  -> similarity score =  0.25687602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Define the model we want to use (it'll download itself)\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "  \"That is a very happy person\",\n",
    "  \"That is a happy dog\",\n",
    "  \"Today is a sunny day\"\n",
    "]\n",
    "\n",
    "# vector embeddings created from dataset\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# query vector embedding\n",
    "query_embedding = model.encode(\"That is a happy person\")\n",
    "\n",
    "# define our distance metric\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "# run semantic similarity search\n",
    "print(\"Query: That is a happy person\")\n",
    "for e, s in zip(embeddings, sentences):\n",
    "    print(s, \" -> similarity score = \",\n",
    "         cosine_similarity(e, query_embedding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c3a86",
   "metadata": {},
   "source": [
    "After installing `NumPy` and `sentence_transformers`, running this script should result in the following calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf909bc4",
   "metadata": {},
   "source": [
    "The results of this script should line up with the results that you see on the [HuggingFace inference API](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) for the model chosen.\n",
    "\n",
    "HuggingFace inference API similarity results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c589daa",
   "metadata": {},
   "source": [
    "![vector_embedding12](../image/vector%20embedding/vector_embedding12.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
